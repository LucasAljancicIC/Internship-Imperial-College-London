{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b109f685",
        "outputId": "24c733db-0c6b-4cc2-ea56-3393d6ce83c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.stattools import kpss\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ],
      "id": "b109f685"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSADmuShm49l"
      },
      "source": [
        "We generate the data for the Lorenz system using the Euler method."
      ],
      "id": "NSADmuShm49l"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vfEGFF9HxnoA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "#Uses Euler method to solve system of differential equations for Lorenz system\n",
        "\n",
        "def Lorenz(T, dt, N_sims,s,r,b):\n",
        "    N_t  = int(T//dt)\n",
        "    sims = np.zeros((N_sims, N_t, 3))\n",
        "    for sim in range(N_sims):\n",
        "        sims[sim,0,0] = 0.\n",
        "        sims[sim,0,1] = 1.\n",
        "        sims[sim,0,2] = 1.05\n",
        "        for i in range(1,N_t):\n",
        "            sims[sim, i, 0] = sims[sim,i-1,0] + dt*(s*(sims[sim,i-1,1]-sims[sim,i-1,0]))\n",
        "            sims[sim, i, 1] = sims[sim,i-1,1] + dt*(r*(sims[sim,i-1,0])-sims[sim,i-1,1] - (sims[sim,i-1,0]*sims[sim,i-1,2]))\n",
        "            sims[sim, i, 2] = sims[sim,i-1,2] + dt*( -b*(sims[sim,i-1,2]) + (sims[sim,i-1,0]*sims[sim,i-1,1]))\n",
        "\n",
        "    return sims.astype(np.float32)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def prepare_data_fast(data_x, delay, normalize, irregular_delays = None):\n",
        "  #Normalize data and take its transpose\n",
        "    data = data_x.T / normalize\n",
        "\n",
        "  # Y = (Y_1,..,Y_N) where Y_k= x_(k+1)\n",
        "\n",
        "    Y = data[delay:]\n",
        "\n",
        "    #interleaves time differences to X\n",
        "    if irregular_delays is not None:\n",
        "        data = np.concatenate((data,irregular_delays[...,None]),-1)\n",
        "\n",
        "  # create array X = (X_1,...X_N) where X_k = (x_k,...,x_k-tau+1)\n",
        "    #X = np.zeros((data.shape[0]-delay,delay*data.shape[1]))\n",
        "    X = np.zeros((data.shape[0]-delay,delay))\n",
        "    for k in range(X.shape[0]):\n",
        "      #X_k = (x_k,...,x_{k+delay-1})\n",
        "        X[k] = data[k:(k+delay)].reshape(-1)\n",
        "\n",
        "    return X, Y"
      ],
      "id": "vfEGFF9HxnoA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFOQnrMAhwrF"
      },
      "source": [
        "We define our kernel"
      ],
      "id": "LFOQnrMAhwrF"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kvNPKFBKxwVr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# Returns the norm of the pairwise difference\n",
        "def norm_matrix(matrix_1, matrix_2):\n",
        "  #Given X = (X_1, ..., X_N), Y = (Y_1, ..., Y_N) output matrix M with entries M_{ij} = ||X_i-Y_j||**2'\n",
        "  #If X and Y matrices then X_1,...X_N and Y_1,...,Y_N are rows of X and Y and M returns squared Euclidean distance between rows of X and Y\n",
        "  #If X and Y are vectors, M returns squared difference between entries of X and Y\n",
        "    #torch.square is the elementwise square of array e.g. torch.square([[2,3],[3,4]]) = [[4,9],[9,16]]\n",
        "    #torch.sum(A,axis=1) is vector containing row sum of matrix elements\n",
        "    #torch.reshape(A,(-1,1)) turns each element of the array into its own block torch.reshape([[1,2]])= [[1],[2]]\n",
        "    norm_square_1 = torch.sum(torch.square(matrix_1), axis = 1)\n",
        "    norm_square_1 = torch.reshape(norm_square_1, (-1,1))\n",
        "\n",
        "    norm_square_2 = torch.sum(torch.square(matrix_2), axis = 1)\n",
        "    norm_square_2 = torch.reshape(norm_square_2, (-1,1))\n",
        "\n",
        "    #matrix_1.shape is tuple containing number of rows and columns of matrix\n",
        "\n",
        "    d1=matrix_1.shape\n",
        "    d2=matrix_2.shape\n",
        "\n",
        "    #if d1 and d2 have different number of columns, take transpose of matrix_1\n",
        "\n",
        "    if d1[1]!=d2[1]:\n",
        "        matrix_1=torch.transpose(matrix_1)\n",
        "\n",
        "    #torch.transpose(matrix_2,0,1) is just usual matrix transpose\n",
        "    #torch.matmul(A,B) is normal matrix multiplication\n",
        "\n",
        "    inner_matrix = torch.matmul(matrix_1, torch.transpose(matrix_2,0,1))\n",
        "\n",
        "    #inner_matrix entries are sum_i sum_j (A_ij B_ji)\n",
        "    # norm_diff gives us -2 sum_i sum_j (A_ij B_ji)  sum_j A_ij^2 + sum_i B_ij^2\n",
        "\n",
        "    norm_diff = -2 * inner_matrix + norm_square_1 + torch.transpose(norm_square_2,0,1)\n",
        "\n",
        "    return norm_diff\n",
        "\n",
        "\n",
        "\n",
        "# Returns the pairwise inner product\n",
        "#e.g. Let X=(X_1,X_2,...,X_N) and Y= (Y_1,Y_2,..,Y_N) be matrices where X_i and Y_i are the rows of X and Y\n",
        "#then the entries of C=inner_matrix(X,Y) are C_ij = np.dot(X_i,Y_j), the dot product of X_i and Y_j\n",
        "def inner_matrix(matrix_1, matrix_2):\n",
        "\n",
        "    d1=matrix_1.shape\n",
        "    d2=matrix_2.shape\n",
        "    #if d1 and d2 have different number of columns, take transpose of matrix_1\n",
        "\n",
        "    if d1[1]!=d2[1]:\n",
        "        matrix_1=torch.transpose(matrix_1,0,1)\n",
        "    return torch.matmul(matrix_1, torch.transpose(matrix_2,0,1))\n",
        "\n",
        "def kernel_anl3(matrix_1, matrix_2, parameters, coefs):\n",
        "    i=0\n",
        "    j=0\n",
        "\n",
        "\n",
        "    matrix = norm_matrix(matrix_1, matrix_2)\n",
        "    matrix[[matrix<0]] = 0\n",
        "    sigma = parameters[i+0]\n",
        "    #torch.exp(matrix)- matrix that returns exponential of each matrix element\n",
        "    K =  torch.exp(-matrix/ (2* sigma**2))\n",
        "    K=K*(coefs[j+0])**2\n",
        "    i=i+1\n",
        "    j=j+1\n",
        "\n",
        "\n",
        "    c = (parameters[i])**2\n",
        "    imatrix = inner_matrix(matrix_1, matrix_2)\n",
        "    K = K+ (coefs[j])**2 *(imatrix+c) ** 2\n",
        "\n",
        "    i=i+1\n",
        "    j=j+1\n",
        "\n",
        "    beta = parameters[i]\n",
        "    gamma = (parameters[i+1])**2\n",
        "    K=K+ (coefs[j])**2 *(beta**2 + gamma*matrix)**(-1/2)\n",
        "\n",
        "    i=i+2\n",
        "    j=j+1\n",
        "\n",
        "    alpha = parameters[i]\n",
        "    beta = parameters[i+1]\n",
        "    K=K+ (coefs[j])**2 *(beta**2 + matrix)**(-alpha)\n",
        "\n",
        "    i=i+2\n",
        "    j=j+1\n",
        "\n",
        "    sigma = parameters[i]\n",
        "    K=K+ (coefs[j])**2 * 1/(1 + matrix/sigma**2)\n",
        "\n",
        "    i=i+1\n",
        "    j=j+1\n",
        "\n",
        "    alpha_0 = parameters[i]\n",
        "    sigma_0 = parameters[i+1]\n",
        "    alpha_1 = parameters[i+2]\n",
        "    sigma_1 = parameters[i+3]\n",
        "\n",
        "    K =  K+ (coefs[j])**2 *alpha_0*torch.maximum(torch.zeros(1,device = parameters.device), 1-matrix/(sigma_0))+ alpha_1 * torch.exp(-matrix/ (2* sigma_1**2))\n",
        "    i=i+4\n",
        "    j=j+1\n",
        "\n",
        "    p = parameters[i]\n",
        "    l = parameters[i+1]\n",
        "    sigma = parameters[i+2]\n",
        "    K =K+ (coefs[j])**2 * torch.exp(-torch.sin(matrix*np.pi/p)**2/l**2)*torch.exp(-matrix/sigma**2)\n",
        "\n",
        "    i=i+3\n",
        "\n",
        "    p = parameters[i]\n",
        "    l = parameters[i+1]\n",
        "    K = K+ (coefs[j])**2 *torch.exp(-torch.sin(matrix*np.pi/p)/l**2)\n",
        "\n",
        "    i=i+2\n",
        "    j=j+1\n",
        "\n",
        "\n",
        "    return K\n",
        "\n",
        "def kernel_matern(matrix_1,matrix_2, parameters):\n",
        "  sigma = parameters[0]\n",
        "  l = parameters[1]\n",
        "  matrix = norm_matrix(matrix_1,matrix_2)\n",
        "  matrix[[matrix<0]] = 0\n",
        "\n",
        "  K = sigma**2*(1+ np.sqrt(5)*torch.sqrt(matrix)/l+5*matrix/(3*l**2))*torch.exp(-np.sqrt(5)*torch.sqrt(matrix)/l)\n",
        "\n",
        "  return torch.Tensor(K)\n",
        "\n",
        "def kernel_linear(matrix_1,matrix_2,parameters):\n",
        "  sigma = parameters[0]\n",
        "  c = parameters[1]\n",
        "  matrix = inner_matrix(matrix_1,matrix_2)\n",
        "\n",
        "  K = sigma**2*(matrix+c)\n",
        "\n",
        "  return torch.Tensor(K)\n"
      ],
      "id": "kvNPKFBKxwVr"
    },
    {
      "cell_type": "code",
      "source": [
        "def Big_kernel(matrix_1, matrix_2, parameters, coefs):\n",
        "\n",
        "  norm = norm_matrix(matrix_1,matrix_2)\n",
        "  norm[[norm<0]] = 0\n",
        "  inner = inner_matrix(matrix_1,matrix_2)\n",
        "  M = torch.zeros(norm.shape)\n",
        "  M[[norm<parameters[33]**2]] = 1\n",
        "  K = coefs[0]**2*(inner+parameters[0]**2)\n",
        "  K +=  coefs[1]**2*torch.pow(parameters[1]**2*inner+parameters[2]**2,torch.abs(parameters[3]))\n",
        "  K += coefs[2]**2*torch.exp(-norm/(2*parameters[4]**2))\n",
        "  K += coefs[3]**2*torch.exp(-torch.sqrt(norm)/(2*parameters[5]**2))\n",
        "  K += coefs[4]**2*torch.exp(-torch.sin(np.pi*norm/parameters[6])**2/parameters[7]**2)*torch.exp(-norm/parameters[8]**2)\n",
        "  K += coefs[5]**2*torch.exp(-torch.sin(np.pi*norm/parameters[9])**2/parameters[10]**2)\n",
        "  K += coefs[6]**2*torch.exp(-torch.sin(np.pi*torch.sqrt(norm)/parameters[11])**2/parameters[12]**2)*torch.exp(-torch.sqrt(norm)/parameters[13]**2)\n",
        "  K += coefs[7]**2*torch.exp(-torch.sin(np.pi*torch.sqrt(norm)/parameters[14])**2/parameters[15]**2)\n",
        "  K += coefs[8]**2*torch.sqrt(norm+parameters[16]**2)\n",
        "  K += coefs[9]**2*torch.pow(parameters[17]**2+parameters[18]**2*norm,-1/2)\n",
        "  K += coefs[10]**2*torch.pow(parameters[19]**2+parameters[20]**2*torch.sqrt(norm),-1/2)\n",
        "  K += coefs[11]**2*torch.pow(parameters[21]**2+torch.sqrt(norm),parameters[22])\n",
        "  K += coefs[12]**2*torch.pow(parameters[23]**2+norm,parameters[24])\n",
        "  K += coefs[13]**2*torch.pow(1+norm/parameters[25],-1)\n",
        "  K = K + coefs[14]**2*(1/(1+torch.sqrt(norm)/parameters[26]**2))\n",
        "  K = K + coefs[15]**2*(1-(1-norm/(norm+parameters[27]**2)))\n",
        "  K = K + coefs[16]**2*torch.maximum(torch.zeros(norm.shape),1-torch.sqrt(norm)/parameters[28]**2)\n",
        "  K = K + coefs[17]**2*torch.maximum(torch.zeros(norm.shape),1-norm/parameters[29]**2)\n",
        "  K = K + coefs[18]**2*(torch.log(torch.sqrt(norm)**parameters[30]+1))\n",
        "  K = K + coefs[19]**2*(torch.tanh(parameters[31]*inner+parameters[32]))\n",
        "  #K = K + coefs[20]**2*(torch.arccos(-torch.sqrt(norm)/parameters[33]**2)-(torch.sqrt(norm)/parameters[33]*torch.sqrt(1-(torch.sqrt(norm)/parameters[33]**2)**2)))*M\n",
        "  return(K)"
      ],
      "metadata": {
        "id": "6R_tV1u00BJ-"
      },
      "id": "6R_tV1u00BJ-",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-XMP-Dmh1fj"
      },
      "source": [],
      "id": "J-XMP-Dmh1fj"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6REeWtetIbvB"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def sample_selection(N, size):\n",
        "  #indices: vectors containing integers from 0 to N-1 inclusive\n",
        "    indices = np.arange(N)\n",
        "  #sample_indices: takes sample of indices (without replacement) and arranges them in ascending order\n",
        "    sample_indices = np.sort(np.random.choice(indices, size, replace= False))\n",
        "    return sample_indices\n",
        "\n",
        "# The pi or selection matrix\n",
        "def pi_matrix(sample_indices, dimension):\n",
        "  #torch.zeros creates tensor of zeros\n",
        "  #dimension is a tuple that contains the number of rows and cols of matrix pi\n",
        "  #.double() converts tensor to float64\n",
        "    pi = torch.zeros(dimension).double()\n",
        "\n",
        "  #In the ith row of pi, replace entries corresponding to ith entries of 'sample_indices' with 1:\n",
        "\n",
        "  #e.g. pi = [[0,0,0,0],[0,0,0,0],[0,0,0,0]] and sample_indices = [0,2,3]\n",
        "  #then pi becomes [[1,0,0,0],[0,0,1,0],[0,0,0,1]]\n",
        "\n",
        "    for i in range(dimension[0]):\n",
        "        pi[i][sample_indices[i]] = 1\n",
        "\n",
        "    return pi\n",
        "\n",
        "def batch_creation(N, batch_size, sample_proportion = 0.5):\n",
        "    #batch_size ==False means we use the full training data in the batch when computing the kernel matrix\n",
        "    if batch_size == False:\n",
        "        batch_indices = np.arange(N)\n",
        "    #if batch_size is between 0 and 1 (that is, a proportion), we use (N*batch_size) % of the training data in our batch\n",
        "    elif 0 < batch_size <= 1:\n",
        "        batch_size = int(N * batch_size)\n",
        "        #returns indices of batch\n",
        "        batch_indices = sample_selection(N, batch_size)\n",
        "    else:\n",
        "    #if batch_size is an integer, we use that number as the number of samples in our batch\n",
        "        batch_indices = sample_selection(N, batch_size)\n",
        "\n",
        "    # Sample from the mini-batch (sample of the sample)\n",
        "    #we use (batch_size*sample_proportion)% of the batch in our mini-batch\n",
        "    sample_size = math.ceil(len(batch_indices)*sample_proportion)\n",
        "    sample_indices = sample_selection(len(batch_indices), sample_size)\n",
        "\n",
        "    return sample_indices, batch_indices\n",
        "\n",
        "class KernelFlows(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, kernel_keyword, nparameters, nfamilies, regu_lambda, regu_lambda2, dim, metric = \"rho_ratio\", batch_size = 100):\n",
        "        super().__init__()\n",
        "\n",
        "        #init gives us definitions we will be using in the rest of the code in class KernelFlows\n",
        "\n",
        "        #e.g. 'rbf', 'kernel_anl3', states the type of kernel we use\n",
        "        self.kernel_keyword = kernel_keyword\n",
        "\n",
        "        self.regu_lambda = regu_lambda\n",
        "\n",
        "        self.regu_lambda2 = regu_lambda2\n",
        "\n",
        "        #we use a vector of ones as our initial guess for the parameters before we learn them\n",
        "        self.kernel_params = torch.nn.Parameter(torch.ones(nparameters),requires_grad = True)\n",
        "        self.coefs = torch.nn.Parameter(torch.ones(nfamilies),requires_grad = True)\n",
        "\n",
        "        #we use the kernel_anl3\n",
        "        self.kernel = kernel_anl3                                ######################## CHOIX DU KERNEL ########################\n",
        "\n",
        "        #dim is dimension of dynamical system\n",
        "        self.dim = dim\n",
        "\n",
        "        #batch_size is the number of samples we use in our batch at each iteration when computing kernel matrix K(X_beta, X_beta) in KF algorithm\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        #type of rho metric we use; we use metric='rho ratio' in our paper\n",
        "\n",
        "        if metric == \"rho_ratio\":\n",
        "            self.rho_fun = self.rho_ratio\n",
        "        elif metric == \"rho_general\":\n",
        "            self.rho_fun = self.rho_general\n",
        "        else:\n",
        "            raise(\"Metric not supported\")\n",
        "        self.metric = metric\n",
        "\n",
        "\n",
        "    def set_training_data(self,X,Y):\n",
        "      #gives us X_train and Y_train\n",
        "        self.X_train = X\n",
        "        self.Y_train = Y\n",
        "\n",
        "\n",
        "    def rho_ratio(self, matrix_data, Y_data, sample_indices,  regu_lambda = 0.000001, regu_lambda2 = 0.000001):\n",
        "        #use CPU for faster matrix computations\n",
        "        self.device = torch.device(\"cpu\")\n",
        "\n",
        "        #K(X,X)\n",
        "\n",
        "        kernel_matrix = self.kernel(matrix_data, matrix_data, self.kernel_params, self.coefs)\n",
        "        #pi_matrix is Boolean matrix..float()\n",
        "        pi = pi_matrix(sample_indices, (sample_indices.shape[0], matrix_data.shape[0])).to(matrix_data.device)\n",
        "\n",
        "        #sample_matrix = K(X_beta,X_beta) (kernel matrix in the denominator of the kernel flows algorithm)\n",
        "        #basically we discard entries of K(X_pi,X_pi) that aren't in the sub-sample and create a new, smaller, matrix K(X_beta,X_beta)\n",
        "        sample_matrix = torch.matmul(pi, torch.matmul(kernel_matrix, torch.transpose(pi,0,1)))\n",
        "\n",
        "        #Y_beta\n",
        "        Y_sample = Y_data[sample_indices]\n",
        "\n",
        "        #(K(X_pi,X_pi)+lambda*I)^-1\n",
        "        inverse_data = torch.linalg.inv(kernel_matrix + regu_lambda * torch.eye(kernel_matrix.shape[0], device = matrix_data.device))\n",
        "        #(K(X_pi,X_pi)+lambda*I)^-1\n",
        "        inverse_sample = torch.linalg.inv(sample_matrix + regu_lambda * torch.eye(sample_matrix.shape[0], device = self.device))\n",
        "        #Y_beta^T (K(X_beta,X_beta)+lambda*I)^-1 Y_beta\n",
        "        Y_data = torch.reshape(Y_data,(matrix_data.shape[0],1))\n",
        "        Y_sample = torch.reshape(Y_sample,(len(sample_indices),1))\n",
        "\n",
        "        top = torch.tensordot(Y_sample, torch.matmul(inverse_sample, Y_sample))\n",
        "        #Y_pi^T (K(X_pi,X_pi)+lambda*I)^-1 Y_pi\n",
        "        bottom = torch.tensordot(Y_data, torch.matmul(inverse_data, Y_data))\n",
        "\n",
        "        # rho = 1 - (Y_beta^T (K(X_beta,X_beta)+lambda*I)^-1 Y_beta)/(Y_pi^T (K(X_pi,X_pi)+lambda*I)^-1 Y_pi)\n",
        "        rho = 1 - top/bottom + self.regu_lambda2*torch.norm(self.coefs,p=1)\n",
        "        #rho = 1 - top/bottom\n",
        "        return rho\n",
        "\n",
        "    def rho_general(self, matrix_data, Y_data,  regu_lambda = 0.000001, **kwargs):\n",
        "        #rho_general is another metric for rho, this time we don't take any sub-batches\n",
        "\n",
        "        #compute K(X,X)\n",
        "\n",
        "        kernel_matrix = self.kernel(matrix_data, matrix_data, self.kernel_params, self.coefs)\n",
        "        #normalize kernel matrix by dividing by the trace\n",
        "\n",
        "        kernel_matrix = kernel_matrix / torch.trace(kernel_matrix)\n",
        "\n",
        "        #compute K(X,X)+lambda*I\n",
        "\n",
        "        inverse_matrix = torch.linalg.inv(kernel_matrix + regu_lambda * torch.eye(kernel_matrix.shape[0]))\n",
        "\n",
        "        #then rho= Y^T  (K(X,X)+lambda*I) Y\n",
        "\n",
        "        rho = torch.tensordot(Y_data, torch.matmul(inverse_matrix, Y_data))\n",
        "\n",
        "        return rho\n",
        "\n",
        "    def forward(self, adaptive_size = False, proportion = 0.5):\n",
        "\n",
        "        if adaptive_size == False or adaptive_size == \"Dynamic\":\n",
        "            sample_size = proportion\n",
        "        else:\n",
        "            print(\"Sample size not recognized\")\n",
        "\n",
        "\n",
        "        # Create a batch and a sample\n",
        "        sample_indices, batch_indices = batch_creation(self.X_train.shape[0], batch_size= self.batch_size, sample_proportion = sample_size)\n",
        "        X_data = self.X_train[batch_indices]\n",
        "        Y_data = self.Y_train[batch_indices]\n",
        "\n",
        "        #optimizer and backward\n",
        "\n",
        "        #calculates rho\n",
        "\n",
        "        rho = self.rho_fun( X_data, Y_data,\n",
        "                                       sample_indices = sample_indices, regu_lambda = self.regu_lambda, regu_lambda2 = self.regu_lambda2)\n",
        "\n",
        "        return rho\n",
        "\n",
        "\n",
        "    def get_parameters(self):\n",
        "      #returns kernel parameters\n",
        "        return self.kernel_params\n",
        "\n",
        "    def get_coefs(self):\n",
        "      #returns kernel parameters\n",
        "        return self.coefs\n",
        "\n",
        "    def compute_kernel_and_inverse(self,regu_lambda = 0.0000001):\n",
        "        X_data = self.X_train\n",
        "        #computes kernel matrix K(X,X) from training data\n",
        "        self.kernel_matrix = self.kernel(X_data,X_data, self.kernel_params, self.coefs)\n",
        "        #adds small nugget term lambda\n",
        "        self.kernel_matrix += regu_lambda * torch.eye(self.kernel_matrix.shape[0], device = X_data.device)\n",
        "        #computes inverse of K+lambda*I\n",
        "        self.inverse_kernel = torch.linalg.inv(self.kernel_matrix)\n",
        "        #computes (K+lambda*I)^-1 Y\n",
        "        self.A_matrix = torch.matmul(self.inverse_kernel,self.Y_train)\n",
        "\n",
        "\n",
        "    def predict(self,x_test):\n",
        "        #kernel_pred is vector K(x,X)\n",
        "        kernel_pred = self.kernel(x_test,self.X_train,self.kernel_params,self.coefs)\n",
        "        #uses representer formula K(x,X)(K+lambda*I)^-1 Y to make prediction\n",
        "        prediction = torch.matmul(kernel_pred,self.A_matrix)\n",
        "        return prediction\n",
        "\n",
        "\n",
        "    def predict_ahead(self,x_test, horizon, delay, delta_t_mode = False, device = torch.device(\"cpu\")):\n",
        "        \"\"\"\n",
        "        Perform n=horizon steps ahead prediction.\n",
        "        If delta_t_mode is True, x_test is expected to have the the following structure (X(t-1),delta_t-1,X(t),delta_t))\n",
        "\n",
        "        out_dim is the dimension of the y vector (and of the observations in x as well)\n",
        "        delay : delay used in the x\n",
        "        \"\"\"\n",
        "        assert horizon >0 # minimum horizon is 1\n",
        "        assert delay >0\n",
        "\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "        #initialize Y_p, the vector that contains our predictions\n",
        "\n",
        "        Y_p = torch.zeros((x_test.shape[0],self.dim))\n",
        "\n",
        "        #converts test data to Pytorch tensor,.double() converts to float, to.device() means we can use GPU\n",
        "        X_test_ = torch.Tensor(x_test).double().to(device)\n",
        "\n",
        "        if delta_t_mode:\n",
        "            indices_delays = [((self.dim+1)*i,(self.dim+1)*i+1) for i in range(delay)] # We should not touch the delta t\n",
        "        else:\n",
        "            indices_delays = [(self.dim*i,self.dim*i+1) for i in range(delay)]\n",
        "\n",
        "        # Make sure there is no contamination (deleting the previous values)\n",
        "        for dim in range(horizon):\n",
        "            n_delays = min(dim,delay)\n",
        "            #for n in range(1,n_delays+1):\n",
        "             #   X_test_[dim::horizon][:,indices_delays[-n]] = 0\n",
        "\n",
        "        # Predicting in chunks and propagating predictions to the next step.\n",
        "        for dim in range(horizon):\n",
        "          #Predict Y_dim, Y_(dim+horizon), Y_(dim+2*horizon) etc. using representer formula\n",
        "          #then update X__(dim+1),..., X_(dim+horizon-1)\n",
        "          #then predict Y_(dim+1), Y_(dim+horizon+1), Y_(dim+2*horizon+1),...,\n",
        "\n",
        "            #Y_p[dim::horizon] = self.predict(X_test_[dim::horizon])\n",
        "            Y_p[dim::horizon] = torch.reshape(self.predict(X_test_[dim::horizon]),Y_p[dim::horizon].shape)\n",
        "            for dim_plus in range(dim+1,min(horizon,delay+dim+1)):\n",
        "                l_x = X_test_[dim_plus::horizon].shape[0]\n",
        "                idx = dim_plus-dim\n",
        "                #now set X_(dim+1) = Y_dim and use that to predict Y_(dim+1) at next iteration etc.\n",
        "                X_test_[dim_plus::horizon][:,indices_delays[-1*idx][0]:2+indices_delays[-1*idx][1]] = Y_p[dim::horizon][:l_x] # should be -1-2:-1 for irregular\n",
        "\n",
        "        return Y_p\n",
        "\n",
        "\n"
      ],
      "id": "6REeWtetIbvB"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "O4XRNJDCj5Ay"
      },
      "outputs": [],
      "source": [
        "def train_kernel(X_train, Y_train, model,  lr = 0.1, epochs=1000,verbose= False):\n",
        "    \"\"\"\n",
        "    dim is the dimension of a single observation\n",
        "    \"\"\"\n",
        "    #the next line states that we are optimizing the kernel parameters using SGD algorithm with learning rate lr\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
        "\n",
        "    #states that we use X_train and Y_train as our training data\n",
        "    #.double() converts tensor elements to float64\n",
        "\n",
        "    model.set_training_data(torch.Tensor(X_train).double(),torch.Tensor(Y_train).double())\n",
        "\n",
        "    #tqdm showa progress bar\n",
        "\n",
        "    for i in tqdm.tqdm(range(epochs)):\n",
        "      #sets gradients of all optimized torch.Tensors to 0\n",
        "        optimizer.zero_grad()\n",
        "        #computes rho\n",
        "        rho = model.forward()\n",
        "        #rho must lie between 0 and 1 inclusive otherwise by definition\n",
        "        if rho>=0 and rho<=1 and model.metric==\"rho_ratio\":\n",
        "          #rho.backward() computes gradient of rho by back-propagation\n",
        "            rho.backward()\n",
        "          #optimizer.step() updates parameter values at each step\n",
        "            optimizer.step()\n",
        "            model.parameters()\n",
        "\n",
        "          #verbose =True would print rho at each epoch\n",
        "            if verbose:\n",
        "                print(rho)\n",
        "\n",
        "    return model"
      ],
      "id": "O4XRNJDCj5Ay"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "56hfq9MzsEY0"
      },
      "outputs": [],
      "source": [
        "def replace_nan_last(array):\n",
        "    #array=array.reshape(1,len(array))\n",
        "    if (array[0]=='NA'):\n",
        "        array[0]=0\n",
        "    for i in range(1,len(array)):\n",
        "        if (array[i]=='NA'):\n",
        "            array[i] = array[i-1]\n",
        "    return array"
      ],
      "id": "56hfq9MzsEY0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5276aca8"
      },
      "source": [
        "## Data Generation"
      ],
      "id": "5276aca8"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "y7J3BJMDm8o7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open('/content/drive/MyDrive/DATA/tsdl.JSON', 'r', encoding='cp1252') as f:    ## YOU HAVE TO CHANGE THIS LINE\n",
        "    data = json.load(f)\n",
        "\n",
        "donnees = []\n",
        "def ts_multi(ind):\n",
        "\n",
        "    error_metrics = []\n",
        "    Data = data[ind]['values']\n",
        "\n",
        "    if data[ind]['type']=='univariate':\n",
        "        Data=replace_nan_last(Data)\n",
        "    else:\n",
        "        Data=[replace_nan_last(Data[i]) for i in range(len(Data)) ]\n",
        "\n",
        "    Data=np.array(Data)\n",
        "    if data[ind]['type']=='univariate':\n",
        "        Data = Data.astype(np.float32)\n",
        "        Data.reshape((len(Data),1))\n",
        "    max_delay = 1\n",
        "    #total samples we use in training+testing\n",
        "    N_points = len(Data)\n",
        "    train_n  = int(len(Data)*0.7)\n",
        "    burnin = 0\n",
        "    dt = 0.01\n",
        "    dt = 1\n",
        "\n",
        "    #time index differences betweeen observations\n",
        "    #create random vector of length N_points-1 where each element lies between 1 and N\n",
        "    delays = np.random.randint(max_delay,size=N_points-1)+1\n",
        "    #observed time indices\n",
        "    #we add np.zeros(1) to indicate that we observe the initial state at time t=0\n",
        "    indices = np.concatenate((np.zeros(1),np.cumsum(delays))).astype(int)\n",
        "    #we add np.zeros(1) to ensure that delays and indices are of the same length\n",
        "    delays = np.concatenate((delays,np.zeros(1))).astype(int)\n",
        "\n",
        "    #final time index\n",
        "    max_idx = indices[-1]\n",
        "    #final time= final time index*step size (dt)\n",
        "    #T = np.ceil(max_idx*dt)\n",
        "\n",
        "    observed_data = Data[indices]\n",
        "\n",
        "    #Use first train_n points of (irregularly sampled) data as training set\n",
        "    train_data = observed_data[:train_n].T\n",
        "\n",
        "    #Use remaining points as test data\n",
        "    test_data = observed_data[train_n:].T\n",
        "    #time indices for train data\n",
        "    delays_train = delays[:train_n]\n",
        "    #time indices for test data\n",
        "    delays_test = delays[train_n:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##### Apply kernel flow from the data\n",
        "\n",
        "    # Some constants\n",
        "    #nparameters=34    ## ANOTHER KERNEL WITH nfamillies = 20\n",
        "    nparameters=26\n",
        "    #delay is tau in the paper (number of samples in X we use)\n",
        "    delay = 10\n",
        "\n",
        "    #nugget term\n",
        "    regu_lambda = 0.001\n",
        "    regu_lambda2=0.0001\n",
        "    #learning rate\n",
        "    lr = 0.01\n",
        "\n",
        "    # Get scaling factor\n",
        "    normalize=np.amax(train_data)\n",
        "\n",
        "    #gives us X_train, Y_train as defined in paper\n",
        "    X_train, Y_train = prepare_data_fast(train_data,delay,normalize)\n",
        "    #gives us X_test, Y_test\n",
        "    X_test, Y_test = prepare_data_fast(test_data,delay,normalize)\n",
        "    dim = 1\n",
        "    #defines the model we are using, family of kernel, number of parameters, nugget term lambda, dimension of dynamical system, optimization metric rho, batch size\n",
        "    model = KernelFlows(\"anl3\", nparameters = nparameters, regu_lambda = regu_lambda, regu_lambda2=regu_lambda2, nfamilies= 8 , dim = dim, metric = \"rho_ratio\", batch_size = 50) #nfamilies=20\n",
        "    #now we train the kernel using this kernel flows model model\n",
        "    model  = train_kernel(X_train, Y_train, model, lr = lr, epochs = 1000)\n",
        "\n",
        "    #computes kernel matrix from learned parameters and inverse (add nugget regu_lambda to ensure numerical stability)\n",
        "    model.compute_kernel_and_inverse(regu_lambda = regu_lambda)\n",
        "    print(model.compute_kernel_and_inverse())\n",
        "\n",
        "    horizon = 2\n",
        "#make predictions within a 20 time step horizon\n",
        "#delta_t_mode=False means we are not incorporating time differences in kernel\n",
        "\n",
        "    Y_pred = model.predict_ahead(X_test,horizon=horizon, delay = delay, delta_t_mode = False)\n",
        "\n",
        "#Mean squared error (mean squared distance from the true value) and R2 statistic (between 0 and 1, measures how well data fits model, )\n",
        "    mse_pred = (Y_pred.detach()-Y_test).pow(2).mean()\n",
        "    r2 = 1-mse_pred/(Y_test.var())\n",
        "    print(f\"MSE On test : {mse_pred:.4f} - R2 : {r2:.4f}\")\n",
        "# SMAPE\n",
        "    numerator = np.abs(Y_pred.detach() - Y_test)\n",
        "    denominator = (np.abs(Y_pred.detach()) + np.abs(Y_test)) / 2\n",
        "    smape = torch.mean(numerator / denominator) *100\n",
        "    print(f\"SMAPE On test : {smape:.4f}\")\n",
        "\n",
        "    def custom_format(value):\n",
        "      if np.isnan(value) or np.isinf(value):\n",
        "        return \"$<$\"\n",
        "      elif abs(value) > 999:\n",
        "        return \"$>>1$\"\n",
        "      else:\n",
        "        return f\"{value:.4f}\"\n",
        "\n",
        "    def custom_format_r2(value):\n",
        "      if np.isnan(value) or np.isinf(value):\n",
        "        return \"$<$\"\n",
        "      elif abs(value) > 1:\n",
        "        return \"$<$\"\n",
        "      else:\n",
        "        return f\"{abs(value):.4f}\"\n",
        "    #donnees.append([ind, f'{mse_pred:.4f}', f'{r2:.4f}', f'{smape:.4f}'])\n",
        "    donnees.append([ind, custom_format(mse_pred), custom_format_r2(r2),custom_format(smape)])\n",
        "    print(model.get_coefs())\n",
        "    print(model.get_parameters())\n",
        "\n",
        "    from pylab import rcParams\n",
        "    from IPython.display import display\n",
        "\n",
        "#Plot predicted time series of x dimension versus truth\n",
        "    if dim == 1 :\n",
        "      \"\"\"fig, ax = plt.subplots(dim,1,figsize=(16,9))\n",
        "      ax.plot(normalize*Y_pred.detach(),label = \"pred\")\n",
        "      ax.plot(normalize*Y_test, label = \"true\")\n",
        "      ax.set_xlabel('Observation')\n",
        "      ax.set_ylabel('x')\n",
        "      ax.set_title(f\"{data[ind]['description']}. Index : {ind}. Taille : {len(data[ind]['values'])}\")\n",
        "      ax.legend(loc = \"upper right\")\n",
        "      print(f\"MSE On test : {mse_pred:.4f} - R2 : {r2:.4f}\")\"\"\"\n",
        "      fig, ax = plt.subplots(dim, 1, figsize=(12, 6 * dim))\n",
        "\n",
        "# Plot the prediction and true values\n",
        "      ax.plot(normalize * Y_pred.detach(), label=\"Prediction\", color='green')\n",
        "      ax.plot(normalize * Y_test, label=\"True\", color='blue')   #Y_test\n",
        "\n",
        "\n",
        "\n",
        "# Customize plot appearance\n",
        "      ax.set_xlabel('Observation')\n",
        "      ax.set_ylabel('Value')\n",
        "      ax.set_title(f\"{data[ind]['description']} - Index: {ind} - Size: {len(data[ind]['values'])}\")\n",
        "      ax.legend(loc=\"upper left\")\n",
        "      ax.grid(True)\n",
        "      x_ticks = range(0, len(Y_test), 10)  # Adjust the interval as needed    #Y_test\n",
        "      ax.set_xticks(x_ticks)\n",
        "      ax.set_xticklabels(x_ticks, rotation=45, ha='right')  # Adjust x-axis tick positions\n",
        "\n",
        "# Format x-axis date labels if dealing with dates (example)\n",
        "# ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "\n",
        "# Adjust layout to prevent overlap of labels\n",
        "      plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "      plt.show()\n",
        "\n",
        "# Print evaluation metrics\n",
        "      #print(f\"MSE on test: {mse_pred:.4f} - R2: {r2:.4f}\")\n",
        "    else :\n",
        "      fig, ax = plt.subplots(dim,1,figsize=(6.4,8.5))\n",
        "      ax[0].plot(normalize*Y_pred.detach(),label = \"pred\")\n",
        "      ax[0].plot(normalize*Y_test, label = \"true\")\n",
        "      ax[0].set_xlabel('Observation')\n",
        "      ax[0].set_ylabel('x')\n",
        "      ax[0].set_title(\"First Dimension\")\n",
        "      ax[0].legend(loc = \"upper right\")\n",
        "      display(fig)\n",
        "      print(f\"MSE On test : {mse_pred:.4f} - R2 : {r2:.4f}\")"
      ],
      "id": "y7J3BJMDm8o7"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "9UDxMzPdqu08",
        "outputId": "9bf1700b-fc5e-4b7b-c575-ed9ff0dfd2e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 48%|████▊     | 482/1000 [00:03<00:03, 154.36it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-63f2bcd82d2f>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mL_sto\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mts_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m# Créez un DataFrame à partir des données\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#table = pd.DataFrame(donnees, columns=['Index', 'MSE', 'R^2', 'SMAPE'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-3203f883f00c>\u001b[0m in \u001b[0;36mts_multi\u001b[0;34m(ind)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKernelFlows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"anl3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregu_lambda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregu_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregu_lambda2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregu_lambda2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfamilies\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"rho_ratio\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#nfamilies=20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m#now we train the kernel using this kernel flows model model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mmodel\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtrain_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m#computes kernel matrix from learned parameters and inverse (add nugget regu_lambda to ensure numerical stability)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-5d60d1448b9b>\u001b[0m in \u001b[0;36mtrain_kernel\u001b[0;34m(X_train, Y_train, model, lr, epochs, verbose)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"rho_ratio\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m           \u001b[0;31m#rho.backward() computes gradient of rho by back-propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mrho\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m           \u001b[0;31m#optimizer.step() updates parameter values at each step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "L_det = [0, 2, 3, 84, 106, 118, 119, 121, 122, 126, 131, 149, 160, 186, 190, 193, 198, 199, 203, 227, 244, 246, 328, 382, 390, 463, 534, 543, 578, 608, 643]\n",
        "L_sto = [4, 6, 9, 78, 105, 112, 113, 130, 136, 145, 158, 171, 212, 215, 218, 221, 231, 232, 248, 254, 260, 263, 273, 282, 296, 314, 338, 352, 547, 615]\n",
        "L_statio =  [89, 90, 91, 171, 195, 208, 214, 217, 227, 229, 230, 231, 232, 247, 254, 273, 293, 314, 315, 338, 352, 375, 452, 502, 539, 547, 610, 612, 615, 623]\n",
        "L_non_statio = [0, 3, 5, 16, 17, 18, 61, 95, 96, 98, 99, 106, 121, 122, 123, 131, 132, 149, 160, 193, 202, 203, 246, 251, 382, 390, 391, 400, 478, 483]\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Créez une liste vide pour stocker les données\n",
        "\n",
        "donnees = []\n",
        "\n",
        "\n",
        "for ind in L_sto:\n",
        "  ts_multi(ind)\n",
        "# Créez un DataFrame à partir des données\n",
        "#table = pd.DataFrame(donnees, columns=['Index', 'MSE', 'R^2', 'SMAPE'])\n",
        "\n",
        "# Affichez la table\n",
        "#print(table)\n",
        "#table.to_csv('table_L_non_statio.csv', index=False)"
      ],
      "id": "9UDxMzPdqu08"
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}